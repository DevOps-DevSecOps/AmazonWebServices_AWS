[pre-requisite]
● AWS CLI
Now configure the 'AWS Access Key ID' and 'AWS Secret access key' using the command "aws configure".
● eksctl
● kubectl

[Commands]
●
$ eksctl version
●
$ eksctl create cluster --name CLUSTER_NAME --region REGION_CODE --fargate
$ eksctl create cluster --name CLUSTER_NAME --region REGION_CODE --node-type INSTANCE_TYPES
  eksctl create cluster --name eks --region us-east-1 --node-type t2.small
$ eksctl create cluster --name=CLUSTER_NAME --region=REGION_CODE --zones=AVAILABILITY_ZONE_1,AVAILABILITY_ZONE_2 --without-nodegroup
$ eksctl create cluster --name CLUSTER_NAME --version VERSION_NO --region REGION_CODE --nodegroup-name NODEGROUP_NAME --node-type INSTANCE_TYPES --nodes NO
  eksctl create cluster --name eks --version 1.27 --region us-east-1 --nodegroup-name eksnodes --node-type m5.large --nodes 2
$ eksctl create cluster --name CLUSTER_NAME --region REGION_CODE --nodegroup-name NODEGROUP_NAME --node-type INSTANCE_TYPES
$ eksctl create cluster --name CLUSTER_NAME --version VERSION_NO --region REGION_CODE --nodegroup-name NODEGROUP_NAME --node-type INSTANCE_TYPES --nodes NO --zones AVAILABILITY_ZONE_1,AVAILABILITY_ZONE_2
  eksctl create cluster --name eks --version 1.27 --region us-west-2 --nodegroup-name eksnodes --node-type m5.large --nodes 2 --zones us-west-2a,us-west-2b,us-west-2c
$ eksctl create cluster --name CLUSTER_NAME --version VERSION_NO --region REGION_CODE --nodegroup-name NODEGROUP_NAME --node-type INSTANCE_TYPES --nodes NO --nodes-min NO --nodes-max NO --ssh-access --ssh-public-key FILE_NAME.pem
  eksctl create cluster --name eks --version 1.27 --region us-east-1 --nodegroup-name eksnodes --node-type m5.large --nodes 2 --nodes-min 2 --nodes-max 4 --ssh-access --ssh-public-key FILE_NAME.pem
$ eksctl create cluster --name=CLUSTER_NAME --version=VERSION_NO --region=REGION_CODE --tags NAME=VALUE --nodes=NO --nodes-min=NO --nodes-max=NO --node-volume-size=NO --node-volume-type=VOLUME_TYPES --ssh-access --ssh-public-key=FILE_NAME.pub
  eksctl create cluster --name=eks --version=1.27 --region=us-west-1 --tags environment=staging --nodes=2 --nodes-min=3 --nodes-max=5 --node-volume-size=50 --node-volume-type=io1 --ssh-access --ssh-public-key=FILE_NAME.pub
●
$ eksctl utils associate-iam-oidc-provider --cluster CLUSTER_NAME --region REGION_CODE --approve
  eksctl utils associate-iam-oidc-provider --cluster eks --region us-east-1 --approve
●
$ eksctl create nodegroup --cluster CLUSTER_NAME --region REGION_CODE --name NODEGROUP_NAME --node-type INSTANCE_TYPES --nodes NO --nodes-min NO --nodes-max NO --node-volume-size NO --ssh-access --ssh-public-key FILE_NAME.pem
  eksctl create nodegroup --cluster eks --region us-east-1 --name eksnodes --node-type m5.large --nodes 2 --nodes-min 2 --nodes-max 4 --node-volume-size 40 --ssh-access --ssh-public-key FILE_NAME.pem
$ eksctl create nodegroup --cluster=CLUSTER_NAME --region=REGION_CODE --name=NODEGROUP_NAME --node-type=INSTANCE_TYPES --nodes=NO --nodes-min=NO --nodes-max=NO --node-volume-size=NO --ssh-access --ssh-public-key=FILE_NAME.pub
  eksctl create nodegroup --cluster=eks --region=us-east-1 --name=eksnodes --node-type=m5.large --nodes=2 --nodes-min=2 --nodes-max=4 --node-volume-size=40 --ssh-access --ssh-public-key=FILE_NAME.pub
● It enable kubectl to communicate with cluster by adding a new context to the kubectl config file.
$ aws eks update-kubeconfig --name CLUSTER_NAME --region REGION_CODE
  aws eks update-kubeconfig --name eks --region us-east-1
Added new context arn:aws:eks:REGION_CODE:AWS_ACCOUNT_ID:cluster/CLUSTER_NAME to /home/USERNAME/.kube/config
Added new context arn:aws:eks:us-east-1:xxxxxxxxxxxxxx:cluster/eks to /home/USERNAME/.kube/config
●
$ eksctl get cluster --region REGION_CODE
  eksctl get cluster --region us-east-1
●
$ eksctl delete nodegroup --cluster CLUSTER_NAME --name NODEGROUP_NAME --region REGION_CODE
  eksctl delete nodegroup --cluster eks --name eksnodes --region us-east-1
$ eksctl delete nodegroup --cluster=CLUSTER_NAME --name=NODEGROUP_NAME --region=REGION_CODE
  eksctl delete nodegroup --cluster=eks --name=eksnodes --region=us-east-1
●
$ eksctl delete cluster --name CLUSTER_NAME
$ eksctl delete cluster --name CLUSTER_NAME --region REGION_CODE
  eksctl delete cluster --name eks --region us-east-1
$ eksctl delete cluster --name=CLUSTER_NAME --region=REGION_CODE
  eksctl delete cluster --name=eks --region=us-east-1
$ eksctl delete cluster CLUSTER_NAME --region REGION_CODE
  eksctl delete cluster eks --region us-east-1
